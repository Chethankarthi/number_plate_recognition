{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the zip file and the destination directory\n",
        "zip_file_path = '/content/drive/MyDrive/SoulPageIT_Assignment/Licplatesdetection_train.zip'  # Update this path to your zip file location\n",
        "extract_dir = '/content/dataset/Licplatesdetection_train/'\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print('Dataset unzipped successfully')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDckDS0hwe_m",
        "outputId": "9915aa16-d400-4392-8a60-4a1d9c990af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset unzipped successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the zip file and the destination directory\n",
        "zip_file_path = '/content/drive/MyDrive/SoulPageIT_Assignment/Licplatesrecognition_train.zip'  # Update this path to your zip file location\n",
        "extract_dir = '/content/dataset/Licplatesrecognition_train/'\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print('Dataset unzipped successfully')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pitrcH5mw6IU",
        "outputId": "9c8617d8-2015-4827-9b52-00f5ea315d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset unzipped successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the zip file and the destination directory\n",
        "zip_file_path = '/content/drive/MyDrive/SoulPageIT_Assignment/test.zip'  # Update this path to your zip file location\n",
        "extract_dir = '/content/dataset/test/'\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print('Dataset unzipped successfully')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki-DlloNw-fx",
        "outputId": "7e3e2fdb-fde3-485a-ea60-84432c888569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset unzipped successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_detection_data(annotations_file, images_dir):\n",
        "    try:\n",
        "        # Load annotations\n",
        "        df = pd.read_csv(annotations_file)\n",
        "\n",
        "        # Prepare image paths and bounding boxes\n",
        "        images = []\n",
        "        bounding_boxes = []\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            image_path = os.path.join(images_dir, row[\"img_id\"])\n",
        "            xmin, ymin, xmax, ymax = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
        "\n",
        "            # Load image\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is None:\n",
        "                print(f\"Error loading image: {image_path}\")\n",
        "                continue\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            image = cv2.resize(image, (224, 224))  # Resize image to 224x224\n",
        "            images.append(image)\n",
        "\n",
        "            # Normalize bounding box coordinates\n",
        "            height, width = image.shape[:2]\n",
        "            xmin_norm = xmin / width\n",
        "            ymin_norm = ymin / height\n",
        "            xmax_norm = xmax / width\n",
        "            ymax_norm = ymax / height\n",
        "            bounding_boxes.append([xmin_norm, ymin_norm, xmax_norm, ymax_norm])\n",
        "\n",
        "        images = np.array(images)\n",
        "        bounding_boxes = np.array(bounding_boxes)\n",
        "\n",
        "        return images, bounding_boxes\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading detection data: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Example usage\n",
        "detection_annotations_file = '/content/Licplatesdetection_train.csv'\n",
        "detection_images_dir = '/content/dataset/Licplatesdetection_train/license_plates_detection_train'\n",
        "\n",
        "detection_images, detection_bboxes = load_detection_data(detection_annotations_file, detection_images_dir)\n"
      ],
      "metadata": {
        "id": "EG2rz1l5knUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def load_recognition_data(annotations_file, images_dir):\n",
        "    try:\n",
        "        # Load annotations\n",
        "        df = pd.read_csv(annotations_file)\n",
        "\n",
        "        # Prepare plate images and texts\n",
        "        plate_images = []\n",
        "        plate_texts = []\n",
        "        max_text_length = 0\n",
        "\n",
        "        # Calculate the maximum length of plate texts\n",
        "        for idx, row in df.iterrows():\n",
        "            plate_text = row['text']\n",
        "            max_text_length = max(max_text_length, len(plate_text))\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            plate_image_path = os.path.join(images_dir, row[\"img_id\"])\n",
        "            plate_text = row['text']\n",
        "\n",
        "            # Load plate image\n",
        "            plate_image = cv2.imread(plate_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "            if plate_image is None:\n",
        "                print(f\"Error loading image: {plate_image_path}\")\n",
        "                continue\n",
        "            plate_image = cv2.resize(plate_image, (100, 50))  # Assuming fixed size for plate images\n",
        "            plate_image = plate_image.astype('float32') / 255.0  # Normalize to [0, 1]\n",
        "            plate_images.append(plate_image)\n",
        "\n",
        "            # Process plate text (convert to categorical labels)\n",
        "            plate_label = [ord(char) - ord('A') for char in plate_text.upper() if char.isalnum()]\n",
        "\n",
        "            # Pad the sequence to the maximum length\n",
        "            padded_plate_label = plate_label + [0] * (max_text_length - len(plate_label))\n",
        "            plate_texts.append(padded_plate_label)\n",
        "\n",
        "        plate_images = np.array(plate_images).reshape(-1, 50, 100, 1)  # Reshape for model input\n",
        "        plate_texts = np.array(plate_texts)\n",
        "\n",
        "        return plate_images, plate_texts\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading recognition data: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Example usage\n",
        "recognition_annotations_file = '/content/Licplatesrecognition_train.csv'\n",
        "recognition_images_dir = '/content/dataset/Licplatesrecognition_train/license_plates_recognition_train'\n",
        "\n",
        "recognition_images, recognition_texts = load_recognition_data(recognition_annotations_file, recognition_images_dir)\n"
      ],
      "metadata": {
        "id": "JI2sekp3vJby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class SimpleDetectionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleDetectionModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 56 * 56, 128)\n",
        "        self.fc2 = nn.Linear(128, 4)  # 4 outputs for bounding box coordinates\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 56 * 56)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the model, criterion and optimizer\n",
        "detection_model = SimpleDetectionModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(detection_model.parameters(), lr=0.001)\n",
        "\n",
        "# Prepare data\n",
        "# Assuming `detection_images` and `detection_bboxes` are loaded and preprocessed correctly\n",
        "detection_images = detection_images.transpose(0, 3, 1, 2)  # Transpose to (batch_size, channels, height, width)\n",
        "detection_images = torch.tensor(detection_images, dtype=torch.float32)\n",
        "detection_bboxes = torch.tensor(detection_bboxes, dtype=torch.float32)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_size = int(0.8 * len(detection_images))\n",
        "val_size = len(detection_images) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(TensorDataset(detection_images, detection_bboxes), [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "detection_model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for images, bboxes in train_loader:\n",
        "        images, bboxes = images.to(device), bboxes.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = detection_model(images)\n",
        "        loss = criterion(outputs, bboxes)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Save the detection model\n",
        "torch.save(detection_model.state_dict(), 'license_plate_detection_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POrdzLyPvO5j",
        "outputId": "825413aa-d466-4f8e-a2a9-04ffe597cd36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 4.2658\n",
            "Epoch [2/10], Loss: 0.9662\n",
            "Epoch [3/10], Loss: 0.3418\n",
            "Epoch [4/10], Loss: 0.1997\n",
            "Epoch [5/10], Loss: 0.3014\n",
            "Epoch [6/10], Loss: 0.2777\n",
            "Epoch [7/10], Loss: 0.2897\n",
            "Epoch [8/10], Loss: 0.2623\n",
            "Epoch [9/10], Loss: 0.2129\n",
            "Epoch [10/10], Loss: 0.2315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dummy data for demonstration\n",
        "recognition_images = torch.randn(900, 1, 50, 100)  # Example shape (900, 1, 50, 100)\n",
        "recognition_texts = torch.randint(0, 36, (900,))  # Example labels (900 samples)\n",
        "\n",
        "# Model definition with adjusted input size\n",
        "class RecognitionModel(nn.Module):\n",
        "    def __init__(self, num_classes=36):\n",
        "        super(RecognitionModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        # Adjusted based on typical pooling effects\n",
        "        self.fc1 = nn.Linear(128 * 6 * 12, 512)  # Adjusted size based on pool size\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "\n",
        "        # Check the size after pooling and adjust accordingly\n",
        "        print(x.size())  # Print size to debug\n",
        "\n",
        "        # Adjust the view size based on actual output size after pooling\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Preprocessing and Dataloaders\n",
        "X_train_rec, X_val_rec, y_train_rec, y_val_rec = train_test_split(recognition_images, recognition_texts, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset_rec = TensorDataset(X_train_rec, y_train_rec)\n",
        "val_dataset_rec = TensorDataset(X_val_rec, y_val_rec)\n",
        "\n",
        "train_loader_rec = DataLoader(train_dataset_rec, batch_size=32, shuffle=True)\n",
        "val_loader_rec = DataLoader(val_dataset_rec, batch_size=32, shuffle=False)\n",
        "\n",
        "# Model Training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "recognition_model = RecognitionModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(recognition_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "recognition_model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader_rec:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = recognition_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader_rec)}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(recognition_model.state_dict(), 'license_plate_recognition_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paQflsKovO2l",
        "outputId": "80ef410a-6507-41a4-cc2d-9bd5854e1ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([16, 128, 6, 12])\n",
            "Epoch 1, Loss: 3.607109080190244\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([16, 128, 6, 12])\n",
            "Epoch 2, Loss: 3.585248501404472\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([16, 128, 6, 12])\n",
            "Epoch 3, Loss: 3.570803300194118\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([16, 128, 6, 12])\n",
            "Epoch 4, Loss: 3.5699966990429424\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([16, 128, 6, 12])\n",
            "Epoch 5, Loss: 3.5699086603911026\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([16, 128, 6, 12])\n",
            "Epoch 6, Loss: 3.5702170496401577\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([16, 128, 6, 12])\n",
            "Epoch 7, Loss: 3.567474883535634\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([16, 128, 6, 12])\n",
            "Epoch 8, Loss: 3.570184054582015\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([16, 128, 6, 12])\n",
            "Epoch 9, Loss: 3.5685312229654063\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([32, 128, 6, 12])\n",
            "torch.Size([16, 128, 6, 12])\n",
            "Epoch 10, Loss: 3.5690754703853442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "egobMjijGxFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define a dataset class for the test images\n",
        "class LicensePlateDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = sorted(os.listdir(root_dir))  # List of image file names\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.image_paths[idx])\n",
        "        image = cv2.imread(img_name)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "        sample = {'image': image, 'img_name': self.image_paths[idx]}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "# Define transformations if needed (e.g., resize, normalization)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert PIL image or numpy ndarray to tensor\n",
        "    # Add other transformations as needed\n",
        "])\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = LicensePlateDataset(root_dir='/content/dataset/test', transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n"
      ],
      "metadata": {
        "id": "y3G1xBzECl0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_paths = os.listdir(root_dir)  # List all image filenames in the directory\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.image_paths[idx])\n",
        "        image = cv2.imread(img_name)\n",
        "\n",
        "        if image is None:\n",
        "            raise ValueError(f\"Failed to read image: {img_name}\")\n",
        "\n",
        "        # Convert BGR to RGB\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Assuming you might need to resize or normalize the image here\n",
        "        # image = cv2.resize(image, (target_size))\n",
        "        # image = image.astype(np.float32) / 255.0\n",
        "\n",
        "        return {'image': image, 'img_name': img_name}\n",
        "\n",
        "# Define your test dataset\n",
        "test_dataset = CustomDataset('/content/dataset/test')\n",
        "\n",
        "# DataLoader for test dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n"
      ],
      "metadata": {
        "id": "M3itIjJPC-qD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for batch in test_loader:\n",
        "    images = batch['image']\n",
        "    image_names = batch['img_name']\n",
        "\n",
        "    for i in range(len(images)):\n",
        "        image = images[i].numpy()  # Convert torch tensor to numpy array\n",
        "        img_name = image_names[i]\n",
        "\n",
        "        # Perform recognition\n",
        "        recognized_text = recognize_license_plate(image)\n",
        "\n",
        "        # Store results\n",
        "        results.append({'image_name': img_name, 'recognized_text': recognized_text})\n",
        "\n",
        "# Print or use results as needed\n",
        "for result in results:\n",
        "    print(f\"Image: {result['image_name']}, Recognized text: {result['recognized_text']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ZwswClQNH6iO",
        "outputId": "fe8c9893-3a77-4e37-d767-3aed9a86b9ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Failed to read image: /content/dataset/test/test",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-8a94d749e345>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimage_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-30cbc9b7d3c8>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to read image: {img_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Convert BGR to RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to read image: /content/dataset/test/test"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define your RecognitionModel class (same as before)\n",
        "class RecognitionModel(nn.Module):\n",
        "    def __init__(self, num_classes=36):\n",
        "        super(RecognitionModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(128 * 6 * 12, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load your trained recognition model\n",
        "recognition_model = RecognitionModel()\n",
        "recognition_model.load_state_dict(torch.load('license_plate_recognition_model.pth'))\n",
        "recognition_model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BimS6uxUHC_V",
        "outputId": "edd9170f-55d5-474b-ea76-c40e53c00b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecognitionModel(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=9216, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=36, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recognize_license_plate(image):\n",
        "    # Preprocess the image (similar to how you did during training)\n",
        "    plate_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    plate_image = cv2.resize(plate_image, (100, 50)).astype('float32') / 255.0\n",
        "    plate_image = torch.Tensor(plate_image).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        text_logits = recognition_model(plate_image)\n",
        "        predicted_text = ''.join([chr(x + ord('A')) for x in text_logits.argmax(dim=1).numpy()])\n",
        "\n",
        "    return predicted_text\n"
      ],
      "metadata": {
        "id": "zpkRn-4kHaD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming test_loader is defined correctly after fixing the image loading issue\n",
        "\n",
        "results = []\n",
        "\n",
        "for batch in test_loader:\n",
        "    images = batch['image']\n",
        "    image_names = batch['img_name']\n",
        "\n",
        "    for i in range(len(images)):\n",
        "        image = images[i].numpy()\n",
        "        img_name = image_names[i]\n",
        "\n",
        "        # Perform recognition\n",
        "        recognized_text = recognize_license_plate(image)\n",
        "\n",
        "        # Store results\n",
        "        results.append({'image_name': img_name, 'recognized_text': recognized_text})\n",
        "\n",
        "# Print or use results as needed\n",
        "for result in results:\n",
        "    print(f\"Image: {result['image_name']}, Recognized text: {result['recognized_text']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "eP7u2VssHCxZ",
        "outputId": "e32898fc-39fd-4dfb-d714-b96a8eb6cbd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading image /content/dataset/test/test: Failed to read image: /content/dataset/test/test\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-a661205364de>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimage_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \"\"\"\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z9d4ROu1HCtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3UJN8YvDHCo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from your_module import DetectionModel, RecognitionModel  # Replace with actual module names\n",
        "\n",
        "# Load detection model\n",
        "detection_model = DetectionModel()\n",
        "detection_model.load_state_dict(torch.load('/content/license_plate_detection_model.pth', map_location=torch.device('cpu')))  # Adjust map_location if necessary\n",
        "detection_model.eval()\n",
        "\n",
        "# Load recognition model\n",
        "recognition_model = RecognitionModel()\n",
        "recognition_model.load_state_dict(torch.load('/content/license_plate_recognition_model.pth', map_location=torch.device('cpu')))  # Adjust map_location if necessary\n",
        "recognition_model.eval()\n",
        "\n",
        "# Example inference function\n",
        "def detect_and_recognize(image):\n",
        "    # Your inference code here\n",
        "    pass\n",
        "\n",
        "# Example usage\n",
        "example_image = detection_images[0]  # Replace with actual image\n",
        "recognized_text = detect_and_recognize(example_image)\n",
        "print(f\"Recognized text: {recognized_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "AQMFE-N29ND9",
        "outputId": "e9962a1d-d59e-4104-9fde-2c9e3320ba44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'your_module'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-34df75f1f20e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myour_module\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetectionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRecognitionModel\u001b[0m  \u001b[0;31m# Replace with actual module names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load detection model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdetection_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDetectionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'your_module'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "czWakLXfvOzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lf6VPHcgvOwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b4Df56dlvOtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5mHeQ-SkvOpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iJHuxeX6vOlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "j_78_sz_oY_i",
        "outputId": "71b84398-4e16-4765-9185-7856801d013f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    img_id      text\n",
            "0    0.jpg  117T3989\n",
            "1    1.jpg  128T8086\n",
            "2   10.jpg   94T3458\n",
            "3  100.jpg  133T6719\n",
            "4  101.jpg   68T5979\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'image_name'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'image_name'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-b08cd00d49ce>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Load and preprocess recognition data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mrecognition_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecognition_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_recognition_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecognition_annotations_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecognition_images_zip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Split data into training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-b08cd00d49ce>\u001b[0m in \u001b[0;36mload_recognition_data\u001b[0;34m(annotations_file, images_zip)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Adjust column names if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mplate_image_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'/tmp/Licplatesrecognition_train/{row[\"image_name\"]}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mplate_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'plate_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'image_name'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Function to build license plate detection model\n",
        "def build_detection_model(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    output_bbox = Dense(4, name='bounding_box')(x)  # Output for bounding box coordinates\n",
        "    model = Model(inputs=input_layer, outputs=output_bbox)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "h78x7XmwoYxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "\n",
        "# Function to build license plate recognition model using BERT\n",
        "def build_recognition_model(num_classes):\n",
        "    # Load BERT tokenizer and model\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Freeze BERT layers\n",
        "    for layer in bert_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Build classification head\n",
        "    input_ids = Input(shape=(128,), dtype=tf.int32)\n",
        "    outputs = bert_model(input_ids)[1]\n",
        "    outputs = Dense(num_classes, activation='softmax')(outputs)\n",
        "\n",
        "    model = Model(inputs=input_ids, outputs=outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "UrRGnkJQoYtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and preprocess detection data\n",
        "detection_images, detection_bboxes = load_detection_data(detection_annotations_file, detection_images_zip)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train_det, X_val_det, y_train_det, y_val_det = train_test_split(detection_images, detection_bboxes, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build and compile detection model\n",
        "detection_model = build_detection_model(input_shape=(224, 224, 3))\n",
        "detection_model.compile(optimizer=Adam(lr=0.001), loss='mse')\n",
        "\n",
        "# Train detection model\n",
        "detection_model.fit(X_train_det, y_train_det, validation_data=(X_val_det, y_val_det), epochs=10, batch_size=32)\n",
        "\n",
        "# Save detection model\n",
        "detection_model.save('license_plate_detection_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "MV-3chaNolEr",
        "outputId": "ab4925cd-3228-48ec-cbcc-04eb2cee4cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'image_name'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'image_name'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-47d17f6f95cd>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load and preprocess detection data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdetection_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_bboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_detection_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetection_annotations_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_images_zip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Split data into training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-921008adbe73>\u001b[0m in \u001b[0;36mload_detection_data\u001b[0;34m(annotations_file, images_zip)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'/tmp/Licplatesdetection_train/{row[\"image_name\"]}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xmin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ymin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xmax'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ymax'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'image_name'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pQ30bpRNolBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XmBtGivfoy1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oVY36FsFoynT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uoAFonIJoyaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uNnnaxw3oyMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wdOclxNhGwL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import cv2\n",
        "\n",
        "# Paths to your provided files\n",
        "detection_annotations_file = '/content/drive/MyDrive/SoulPageIT_Assignment/Licplatesdetection_train.csv'\n",
        "detection_images_zip = '/content/drive/MyDrive/SoulPageIT_Assignment/Licplatesdetection_train.zip'\n",
        "\n",
        "# Function to load and preprocess Training Set 1 (vehicle images with bounding boxes)\n",
        "def load_detection_data(annotations_file, images_zip):\n",
        "    # Load annotations\n",
        "    df = pd.read_csv(annotations_file)\n",
        "\n",
        "    # Extract images from zip file\n",
        "    with zipfile.ZipFile(images_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/tmp/Licplatesdetection_train')\n",
        "\n",
        "    # Prepare image paths and bounding boxes\n",
        "    images = []\n",
        "    bounding_boxes = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        image_path = f'/tmp/Licplatesdetection_train/{row[\"image_name\"]}'\n",
        "        xmin, ymin, xmax, ymax = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
        "\n",
        "        # Load image\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        images.append(image)\n",
        "\n",
        "        # Normalize bounding box coordinates\n",
        "        width, height = image.shape[1], image.shape[0]\n",
        "        xmin_norm = xmin / width\n",
        "        ymin_norm = ymin / height\n",
        "        xmax_norm = xmax / width\n",
        "        ymax_norm = ymax / height\n",
        "        bounding_boxes.append([xmin_norm, ymin_norm, xmax_norm, ymax_norm])\n",
        "\n",
        "    images = np.array(images)\n",
        "    bounding_boxes = np.array(bounding_boxes)\n",
        "\n",
        "    return images, bounding_boxes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to your provided files\n",
        "recognition_annotations_file = '/content/drive/MyDrive/SoulPageIT_Assignment/Licplatesrecognition_train.csv'\n",
        "recognition_images_zip = '/content/drive/MyDrive/SoulPageIT_Assignment/Licplatesrecognition_train.zip'\n",
        "\n",
        "# Function to load and preprocess Training Set 2 (license plate images with text annotations)\n",
        "def load_recognition_data(annotations_file, images_zip):\n",
        "    # Load annotations\n",
        "    df = pd.read_csv(annotations_file)\n",
        "\n",
        "    # Extract images from zip file\n",
        "    with zipfile.ZipFile(images_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/tmp/Licplatesrecognition_train')\n",
        "\n",
        "    # Prepare plate images and texts\n",
        "    plate_images = []\n",
        "    plate_texts = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        plate_image_path = f'/tmp/Licplatesrecognition_train/{row[\"image_name\"]}'\n",
        "        plate_text = row['plate_text']\n",
        "\n",
        "        # Load plate image\n",
        "        plate_image = cv2.imread(plate_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        plate_image = cv2.resize(plate_image, (100, 50))  # Assuming fixed size for plate images\n",
        "        plate_image = plate_image.astype('float32') / 255.0  # Normalize to [0, 1]\n",
        "        plate_images.append(plate_image)\n",
        "\n",
        "        # Process plate text (convert to categorical labels)\n",
        "        plate_label = []\n",
        "        for char in plate_text:\n",
        "            if char.isalnum():\n",
        "                if char.isdigit():\n",
        "                    plate_label.append(ord(char) - ord('0'))\n",
        "                else:\n",
        "                    plate_label.append(ord(char.upper()) - ord('A') + 10)\n",
        "\n",
        "        plate_texts.append(plate_label)\n",
        "\n",
        "    plate_images = np.array(plate_images).reshape(-1, 50, 100, 1)  # Reshape for model input\n",
        "    plate_texts = np.array(plate_texts)\n",
        "\n",
        "    return plate_images, plate_texts\n"
      ],
      "metadata": {
        "id": "4ThGH0sLjFwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Function to build license plate detection model\n",
        "def build_detection_model(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    output_bbox = Dense(4, name='bounding_box')(x)  # Output for bounding box coordinates\n",
        "    model = Model(inputs=input_layer, outputs=output_bbox)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "roDo3qDOkivp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to build license plate recognition model\n",
        "def build_recognition_model(input_shape, num_classes):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "GEE8t8PwkysV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to train license plate detection model\n",
        "def train_detection_model(images, bounding_boxes):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(images, bounding_boxes, test_size=0.2, random_state=42)\n",
        "\n",
        "    detection_model = build_detection_model(input_shape=(224, 224, 3))\n",
        "    detection_model.compile(optimizer=Adam(lr=0.001), loss='mse')\n",
        "    detection_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
        "    detection_model.save('license_plate_detection_model.h5')\n"
      ],
      "metadata": {
        "id": "zFSaN65Ak3OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train license plate recognition model\n",
        "def train_recognition_model(images, labels):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    recognition_model = build_recognition_model(input_shape=(50, 100, 1), num_classes=36)  # Assuming 36 classes (26 letters + 10 digits)\n",
        "    recognition_model.compile(optimizer=Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    recognition_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
        "    recognition_model.save('license_plate_recognition_model.h5')\n"
      ],
      "metadata": {
        "id": "3iLHLJcak7Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to evaluate license plate detection model\n",
        "def evaluate_detection_model(model, X_val, y_val):\n",
        "    y_pred = model.predict(X_val)\n",
        "    mse = mean_squared_error(y_val, y_pred)\n",
        "    print(f\"Mean Squared Error (MSE) for bounding boxes: {mse}\")\n",
        "\n",
        "    # Optionally, visualize some predictions\n",
        "    for i in range(min(5, len(X_val))):\n",
        "        image = X_val[i]\n",
        "        true_bbox = y_val[i]\n",
        "        pred_bbox = y_pred[i]\n",
        "\n",
        "        # Visualize image with true and predicted bounding boxes\n",
        "        # (Code for visualization depends on your specific implementation)\n"
      ],
      "metadata": {
        "id": "66qGCYLfllk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate license plate recognition model\n",
        "def evaluate_recognition_model(model, X_val, y_val):\n",
        "    loss, accuracy = model.evaluate(X_val, y_val)\n",
        "    print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
        "\n",
        "    # Optionally, print and analyze classification report or confusion matrix\n",
        "    # (Code for additional evaluation metrics depends on your specific implementation)\n"
      ],
      "metadata": {
        "id": "-k3Ac69UmdRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Function to load test set images\n",
        "def load_test_data(test_zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(test_zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall('/tmp/test')\n",
        "\n",
        "        # List test image files\n",
        "        test_image_files = os.listdir('/tmp/test')\n",
        "        test_image_files.sort()  # Ensure sorted order for consistency\n",
        "\n",
        "        test_images = []\n",
        "        for image_file in test_image_files:\n",
        "            image_path = os.path.join('/tmp/test', image_file)\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is None:\n",
        "                print(f'Failed to load image: {image_path}')\n",
        "                continue\n",
        "\n",
        "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "            test_images.append(image_rgb)\n",
        "\n",
        "        test_images = np.array(test_images)\n",
        "        return test_images, test_image_files\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error loading test data: {str(e)}')\n",
        "        return None, None\n"
      ],
      "metadata": {
        "id": "OxGgewccmhSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your provided test.zip file\n",
        "test_zip_path = '/content/drive/MyDrive/SoulPageIT_Assignment/test.zip'\n",
        "\n",
        "# Load test images\n",
        "test_images, test_image_files = load_test_data(test_zip_path)\n",
        "\n",
        "if test_images is not None:\n",
        "    print(f'Loaded {len(test_images)} test images successfully.')\n",
        "    # Continue with model loading and inference\n",
        "else:\n",
        "    print('Failed to load test images. Check error messages for details.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0KPXAUIk_7e",
        "outputId": "ca5554da-9e9c-48eb-9c34-29261fce37c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to load image: /tmp/test/test\n",
            "Loaded 0 test images successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "oR518uTtlQoI",
        "outputId": "53ae9785-5efa-4e38-91be-ed7776089c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "No file or directory found at license_plate_detection_model.h5",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a75e8b9fd630>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load license plate detection model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdetection_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'license_plate_detection_model.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load license plate recognition model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    235\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                             )\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at license_plate_detection_model.h5"
          ]
        }
      ]
    }
  ]
}